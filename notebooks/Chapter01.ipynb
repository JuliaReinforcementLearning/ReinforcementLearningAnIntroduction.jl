{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Status\u001b[22m\u001b[39m `/mnt/E4E0A9C0E0A998F6/github/ReinforcementLearningAnIntroduction.jl/notebooks/Project.toml`\n",
      " \u001b[90m [02c1da58]\u001b[39m\u001b[37m RLIntro v0.2.0 [`..`]\u001b[39m\n",
      " \u001b[90m [158674fc]\u001b[39m\u001b[37m ReinforcementLearning v0.4.0 [`../../ReinforcementLearning.jl`]\u001b[39m\n",
      " \u001b[90m [25e41dd2]\u001b[39m\u001b[37m ReinforcementLearningEnvironments v0.1.1\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "]st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling ReinforcementLearning [158674fc-8238-5cab-b5ba-03dfc80d1318]\n",
      "└ @ Base loading.jl:1264\n",
      "WARNING: using DataStructures.capacity in module ReinforcementLearning conflicts with an existing identifier.\n",
      "WARNING: using DataStructures.update! in module ReinforcementLearning conflicts with an existing identifier.\n",
      "WARNING: using DataStructures.isfull in module ReinforcementLearning conflicts with an existing identifier.\n",
      "WARNING: using ProgressMeter.update! in module ReinforcementLearning conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "___\n",
       "___\n",
       "___\n",
       "isdone = [false], winner = [nothing]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ReinforcementLearning, ReinforcementLearningEnvironments, RLIntro\n",
    "using RLIntro.TicTacToe\n",
    "\n",
    "env = TicTacToeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5478, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nstates, nactions = length(observation_space(env)), length(action_space(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious why there are `5478` states, you may see the discussions [here](https://math.stackexchange.com/questions/485752/tictactoe-state-space-choose-calculation/485852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Observation{Float64,Bool,Int64,NamedTuple{(:legal_actions,),Tuple{Array{Bool,1}}}}(0.0, false, 4175, (legal_actions = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observe(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the Monte Carlo based method to estimate the value of each state for each player. Think about this, if we have the precise estimation of each state after taking some specific observation according to current observation, then we can just choose the action that leads to the maximum estimation.\n",
    "\n",
    "Let's create a value approximator first (here we use the `TabularVApproximator` defined in `ReinforcementLearning.jl`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularVApproximator([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1 = TabularVApproximator(nstates)\n",
    "V2 = TabularVApproximator(nstates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by default all the estimations are initialed with `0.0`. Usually it won't be a problem, but here we can initialize it with a better starting point. For each state, we can check that if the state is a final state and set the initial estimation accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_V! (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function init_V!(V, role)\n",
    "    for i in 1:length(V.table)\n",
    "        s = TicTacToe.ID2STATE[i]\n",
    "        isdone, winner = TicTacToe.STATES_INFO[s]\n",
    "        if isdone\n",
    "            if winner === nothing\n",
    "                V.table[i] = 0.5\n",
    "            elseif winner === role\n",
    "                V.table[i] = 1.\n",
    "            else\n",
    "                V.table[i] = 0.\n",
    "            end\n",
    "        else\n",
    "            V.table[i] = 0.5\n",
    "        end\n",
    "    end\n",
    "    V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularVApproximator([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5  …  0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_V!(V1, TicTacToe.offensive)\n",
    "init_V!(V2, TicTacToe.defensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct a `MonteCarloLearner` for each player. Here the `MonteCarloLearner` is just a wrapper around the approximator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonteCarloLearner{:EveryVisit,TabularVApproximator,CachedSampleAvg}(TabularVApproximator([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5  …  0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), 1.0, 0.1, CachedSampleAvg(Dict{Any,Any}()))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner_1 = MonteCarloLearner(V1; α=0.1, kind=:EveryVisit)\n",
    "learner_2 = MonteCarloLearner(V2; α=0.1, kind=:EveryVisit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will create the `MonteCarloAgent`. To create such an agent, we need to provide a `learner` and a `policy`. We already have the learners above. Now let's create a policy.\n",
    "\n",
    "A policy is a mapping from states to actions. Considering that we already have the estimations of states, a simple policy would be checking the estimation of the following up states and select one action which will result to the best state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_policy (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function create_policy(V, role, ϵ=0.01)\n",
    "    obs -> begin\n",
    "        legal_actions, state = findall(get_legal_actions(obs)), get_state(obs)\n",
    "        next_states = TicTacToe.get_next_states(TicTacToe.ID2STATE[state], role, legal_actions)\n",
    "        next_state_estimations = [V(TicTacToe.STATE2ID[ns]) for ns in next_states]\n",
    "        max_val, idx = findmax(next_state_estimations)\n",
    "        rand() < ϵ ? rand(legal_actions) : legal_actions[idx]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#7 (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "π_1 = create_policy(V1, TicTacToe.offensive)\n",
    "π_2 = create_policy(V2, TicTacToe.defensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonteCarloAgent{MonteCarloLearner{:EveryVisit,TabularVApproximator,CachedSampleAvg},var\"##7#9\"{TabularVApproximator,RLIntro.TicTacToe.Defensive,Float64},EpisodeTurnBuffer{(:reward, :terminal, :state, :action),Tuple{Float64,Bool,Int64,Int64},NamedTuple{(:reward, :terminal, :state, :action),Tuple{Array{Float64,1},Array{Bool,1},Array{Int64,1},Array{Int64,1}}}},RLIntro.TicTacToe.Defensive,ReinforcementLearning.TrainingMode}(O, ReinforcementLearning.TrainingMode(), MonteCarloLearner{:EveryVisit,TabularVApproximator,CachedSampleAvg}(TabularVApproximator([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5  …  0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), 1.0, 0.1, CachedSampleAvg(Dict{Any,Any}())), var\"##7#9\"{TabularVApproximator,RLIntro.TicTacToe.Defensive,Float64}(TabularVApproximator([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5  …  0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), O, 0.01), NamedTuple{(:reward, :terminal, :state, :action),Tuple{Float64,Bool,Int64,Int64}}[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_1 = MonteCarloAgent(learner_1, π_1, episode_RTSA_buffer();role=TicTacToe.offensive);\n",
    "agent_2 = MonteCarloAgent(learner_2, π_2, episode_RTSA_buffer();role=TicTacToe.defensive);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{EmptyHook,1}:\n",
       " EmptyHook()\n",
       " EmptyHook()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run((agent_1, agent_2), env, StopAfterStep(1000000; is_show_progress=false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonteCarloAgent{MonteCarloLearner{:EveryVisit,TabularVApproximator,CachedSampleAvg},var\"##7#9\"{TabularVApproximator,RLIntro.TicTacToe.Defensive,Float64},EpisodeTurnBuffer{(:reward, :terminal, :state, :action),NTuple{4,Tuple{Float64,Bool,Int64,Int64}},NamedTuple{(:reward, :terminal, :state, :action),NTuple{4,Array{Tuple{Float64,Bool,Int64,Int64},1}}}},RLIntro.TicTacToe.Defensive,ReinforcementLearning.EvaluatingMode}(O, ReinforcementLearning.EvaluatingMode(), MonteCarloLearner{:EveryVisit,TabularVApproximator,CachedSampleAvg}(TabularVApproximator([0.5, 0.55, 0.5, 0.5, 0.0018793574018245605, 0.5, 0.5, 0.5, 0.55, 0.5  …  0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), 1.0, 0.1, CachedSampleAvg(Dict{Any,Any}(4460 => SampleAvg(335, 0.5179104477611935),1131 => SampleAvg(34, 0.0),3532 => SampleAvg(234, 0.4914529914529915),2126 => SampleAvg(2, 1.0),4809 => SampleAvg(7, 0.4285714285714286),1461 => SampleAvg(70, 0.49999999999999983),3335 => SampleAvg(85, 0.0),4156 => SampleAvg(2, 0.0),1124 => SampleAvg(1, 1.0),2061 => SampleAvg(3, 0.0)…))), var\"##7#9\"{TabularVApproximator,RLIntro.TicTacToe.Defensive,Float64}(TabularVApproximator([0.5, 0.55, 0.5, 0.5, 0.0018793574018245605, 0.5, 0.5, 0.5, 0.55, 0.5  …  0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]), O, 0.0), NamedTuple{(:reward, :terminal, :state, :action),NTuple{4,Tuple{Float64,Bool,Int64,Int64}}}[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_agent_1 = similar(agent_1; mode = RL.EVALUATING_MODE, π=create_policy(V1, agent_1.role, 0.0));\n",
    "eval_agent_2 = similar(agent_2; mode = RL.EVALUATING_MODE, π=create_policy(V2, agent_2.role, 0.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook = (TotalRewardPerEpisode(;tag=\"Evaluating\"), TotalRewardPerEpisode(;tag=\"Evaluating\"))\n",
    "\n",
    "run((eval_agent_1, eval_agent_2), env, StopAfterEpisode(1);hook=hook)  # the policy is deterministic now, so one episode will be enough.\n",
    "\n",
    "sum(hook[1].rewards .== 0.5), sum(hook[2].rewards .== 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to play this game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "play (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function read_action_from_stdin()\n",
    "    print(\"Your input:\")\n",
    "    input = parse(Int, readline())\n",
    "    !in(input, 1:9) && error(\"invalid input!\")\n",
    "    input\n",
    "end\n",
    "\n",
    "function play()\n",
    "    env = TicTacToeEnv()\n",
    "    println(\"\"\"You play first!\n",
    "    1 4 7\n",
    "    2 5 8\n",
    "    3 6 9\"\"\")\n",
    "    while true\n",
    "        action = read_action_from_stdin()\n",
    "        env(action)\n",
    "        println(env)\n",
    "        obs = observe(env, TicTacToe.offensive)\n",
    "        if get_terminal(obs)\n",
    "            if get_reward(obs) == 0.5\n",
    "                println(\"Tie!\")\n",
    "            elseif get_reward(obs) == 1.0 \n",
    "                println(\"You win!\")\n",
    "            else\n",
    "                println(\"Invalid input!\")\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "\n",
    "        env(eval_agent_2(observe(env)))\n",
    "        println(env)\n",
    "        obs = observe(env, TicTacToe.defensive)\n",
    "        if get_terminal(obs)\n",
    "            if get_reward(obs) == 0.5\n",
    "                println(\"Tie!\")\n",
    "            elseif get_reward(obs) == 1.0 \n",
    "                println(\"Your lose!\")\n",
    "            else\n",
    "                println(\"You win!\")\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You play first!\n",
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n",
      "Your input:stdin> 3\n",
      "___\n",
      "___\n",
      "X__\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "___\n",
      "_O_\n",
      "X__\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "Your input:stdin> 9\n",
      "___\n",
      "_O_\n",
      "X_X\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "___\n",
      "_O_\n",
      "XOX\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "Your input:stdin> 2\n",
      "___\n",
      "XO_\n",
      "XOX\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "_O_\n",
      "XO_\n",
      "XOX\n",
      "isdone = [true], winner = [O]\n",
      "\n",
      "Your lose!\n"
     ]
    }
   ],
   "source": [
    "play()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.0-rc1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
